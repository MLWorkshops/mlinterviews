{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f865db3",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial: An Introduction to Neural Networks\n",
    "\n",
    "In this tutorial you are going to learn the basics of neural networks. We will walk through each of the different parts that make up a neural network. In the end, we will put these parts together to create a model that can classify animals from a zoo dataset. The zoo dataset was taken from [KEEL](https://sci2s.ugr.es/keel/category.php?cat=clas&order=clasR#sub2).\n",
    "\n",
    "The model will be developed from first principles using `Numpy`. This means we can see the inner workers of the neural network and how each part works together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0365a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf4169",
   "metadata": {},
   "source": [
    "## What is a neural network?\n",
    "So, what is a neural network. A neural network can be thought of as a more complicated linear equation.\n",
    "\n",
    "$y = m*x + b$\n",
    "\n",
    "Given some input (`x`), the model applies some weights (`m`) and biases (`b`) to predict an outcome, `y`.\n",
    "\n",
    "But, why is it called 'neural'? That seems to imply something related to the brain. For that, we need to introduce the perceptron.\n",
    "\n",
    "## What is a perceptron?\n",
    "The perceptron is the most basic unit of a neural network. It functions much like a neuron in our brain. Each neuron in our brain receives signals from dendrites. Depending on the signals received, the neuron will fire or remain quite.\n",
    "\n",
    "The perceptron functions in much the same way. Each perceptron receives inputs from adjoining perceptrons. It will then combine these inputs using a weighted sum function and output a value.  The output value then passes through an activation function.\n",
    "\n",
    "## Activate Function\n",
    "Activation functions insert non-linearities into the neural network. The force the output of the perceptrons to be in a certain range. These functions allow the neural network the make decisions. Let's take a look at a couple examples.\n",
    "\n",
    "### Sigmoid Activation\n",
    "This activation function sets all inputs to values between 0 and 1. It does this using an exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325219ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(Z):\n",
    "    activation = 1/(1 + np.exp(-1*Z))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1783e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.arange(21) - 10\n",
    "sigmoid = sigmoid_activation(Z)\n",
    "plt.plot(Z, sigmoid)\n",
    "plt.title('Sigmoid Activation Function')\n",
    "plt.xlabel('Z')\n",
    "plt.ylabel('sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4356a4",
   "metadata": {},
   "source": [
    "### Relu Activation\n",
    "This activation function sets all negative values to 0 and otherwise returns the positive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(Z):\n",
    "    activation = np.maximum(0.0, Z)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b879a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.arange(21) - 10\n",
    "relu = relu_activation(Z)\n",
    "plt.plot(Z, relu)\n",
    "plt.title('Relu Activation Function')\n",
    "plt.xlabel('Z')\n",
    "plt.ylabel('relu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb41d8",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "In our model, we are going to perform classification. Therefore, the neural network needs a way to predict a class as output, given some input features. This can be achieved by using a softmax function, which assigns a probability to each class. All probabibilities add up to one. The class with the highest probability is assigned the prediction for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z))\n",
    "    activation = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad70feb",
   "metadata": {},
   "source": [
    "### Connecting Perceptrons\n",
    "Now that we have the 'neurons' (perceptrons) of the neural network, we need to create the network by connecting perceptrons together. Neurons are aligned in layers. The features are received as inputs to the initial layer. The final layer returns a prediction for the model. In between are hidden layers. They are called hidden because we do not have access to information passed to these neurons. The hidden layers allow the neural network to learn complex relationships between the input features to help it make predictions. See Figure 1 for an example of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d46314",
   "metadata": {},
   "source": [
    "<img src=\"https://freecontent.manning.com/wp-content/uploads/duerr_NNA_01.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<div style=\"width: 800px; margin: 2px auto\">\n",
    "<span>Figure 1: Example of a neural network.</span>\n",
    "<br>\n",
    "<span style='font-size: 10px;'>Image from <a href=\"https://freecontent.manning.com/neural-network-architectures/\">Neural Networks Archictures</a></span>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce1bf5",
   "metadata": {},
   "source": [
    "Let's define the architecture that we will use for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 20\n",
    "NUM_CLASSES = 7\n",
    "LAYER_SIZES = [25, 25]  # TASK: Play with the number of neurons per layer as well as the number of layers.\n",
    "\n",
    "LAYER_SIZES.insert(0, NUM_FEATURES)\n",
    "\n",
    "LAYER_SIZES.append(NUM_CLASSES)\n",
    "\n",
    "LAYER_ACTIVATIONS = ['relu', 'relu', 'softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28133bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The layer sizes include: {LAYER_SIZES}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ad52e",
   "metadata": {},
   "source": [
    "Out dataset contains 20 features, so that is the input. The first and second hidden layers contain 25 neurons. Adding more elements to this list will increase the number of hidden layers. The final layer is the softmax layer, with seven outputs, one of each type of animal being predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49ceed",
   "metadata": {},
   "source": [
    "### How are values passed between layers?\n",
    "\n",
    "Mathematically, this is done by using a weighted sum function. For each layer, the inputs to the perceptrons are multiplied by a weight. The weighted inputs are then added together. Finally, a bias term is added to the sum. This can be done exhaustively. But, a more elegant way is using linear algebra. Here is an example using numpy:\n",
    "\n",
    "`Z = np.dot(np.transpose(W), X) + b`\n",
    "\n",
    "`Z` is now a vector of size equal to the number of neurons in the layer. The vector `Z` now passes through an activation function before being passed to the next layer in the neural network.\n",
    "\n",
    "### Layer Initialization\n",
    "One of the most import steps in setting up a neural network is initializing the weights and biases. Basically, the neural network needs a starting point at which to begin it's learning process. The choice of initial values is very important. If all values are the same, then the outputs to each hidden layer will be the same. This will prevent each neuron in the hidden layer from learning anything useful. To prevent this problem, the weights and biases are initialized to small values. Let's initialize our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33000c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network():\n",
    "    architecture = {}\n",
    "    for layer in range(1, len(LAYER_SIZES)):\n",
    "        architecture[f'layer_{layer}'] = {\n",
    "            'W': np.random.randn(LAYER_SIZES[layer],\n",
    "                                 LAYER_SIZES[layer-1]) * 0.1,\n",
    "            'b': np.random.randn(LAYER_SIZES[layer], 1) * 0.1,\n",
    "            'activation': LAYER_ACTIVATIONS[layer-1]\n",
    "        }\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30817bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = initialize_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240ffc8",
   "metadata": {},
   "source": [
    "### Single Forward Pass\n",
    "Ok, now we have defined our neural network architecture and have initialized all weights and biases. We now need to define a function that will pass values from layer to layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_map = {\n",
    "    'sigmoid': sigmoid_activation,\n",
    "    'relu': relu_activation,\n",
    "    'softmax': softmax_activation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_forward_pass(A_previous, W, b, activation):\n",
    "    try:\n",
    "        act_function = act_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The activation {activation} is not recognized.\\nIt must be one of the following: {list(act_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    Z = np.dot(W, A_previous) + b\n",
    "    A = act_function(Z)\n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb0015",
   "metadata": {},
   "source": [
    "This function uses linear algebra to perform the weighted sum for each layer that was discussed above. It outputs the activated outputs `A` and the non-activated outputs `Z`.\n",
    "\n",
    "### Full Forward Pass\n",
    "In the next function I define, we loop through each layer in the network and perform a forward pass using `single_forward_pass()`. A dictionary is used to store all activate and non-activated values in a cache for later use. The final activation value as well as the cache are output. After the inputs have been passed in, the final output will be the predictions for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_pass(X, network):\n",
    "    \n",
    "    cache = {}\n",
    "    A = np.transpose(X)\n",
    "    \n",
    "    for layer in range(1, len(network) + 1):\n",
    "        A_previous = A\n",
    "        A, Z = single_forward_pass(A_previous,\n",
    "                                   network[f'layer_{layer}']['W'], \n",
    "                                   network[f'layer_{layer}']['b'], \n",
    "                                   network[f'layer_{layer}']['activation'])\n",
    "        \n",
    "        cache[f'A_{layer-1}'] = A_previous\n",
    "        cache[f'Z_{layer}'] = Z\n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607648b",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "After making predictions, we need a way to evaluate the model. We would like to know how well the model can predict each label. The simplest method is to compute the accuracy. For each prediction, compare it to the true value. Then count up the fraction of correct predictions. Here is a function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    y_pred_transpose = np.transpose(y_pred)\n",
    "    y_pred_flat = np.argmax(y_pred_transpose, 1)\n",
    "    y_flat = np.argmax(y, 1)\n",
    "    accuracy = np.mean(y_pred_flat == y_flat)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff8e4ec",
   "metadata": {},
   "source": [
    "Note: You will notice that periodically I will using `np.transpose()`. This will ensure that when comparing predictions and labels, each will have the same shape.\n",
    "\n",
    "### Cross Entropy Cost Function\n",
    "While accuracy can provide a quick way to assess model performance, it does not provide much information on how well the model does at predicting each of the different classes. As shown above, the output of the softmax function will produce probabilities for each class for each sample in the training set. We can compare the probabilities to the actual values using a cross entropy function. Here is the formula:\n",
    "\n",
    "$cost = -1 * \\sum \\limits _{i} ^m y_{i} * log(y'_{i})$\n",
    "\n",
    "where $y$ is the true value and $y'$ is the prediction.\n",
    "\n",
    "Here is how we can code up cross entropy in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e53b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_cost(y_pred, y):\n",
    "    \n",
    "    cost = -1*np.mean(y * np.log(np.transpose(y_pred)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e77128",
   "metadata": {},
   "source": [
    "Great! Now we have a way to evaluate how well out model is at classification. But, our model will be useless if it can't learn anything. We need it to be able to decide how to update it's weights and biases in order to improve it's ability to make predictions.\n",
    "\n",
    "### Gradients\n",
    "During training, we want our model to search for the best fit. This will be the combination of parameters that yields the smallest cost function. In simplest terms, this search space can be thought of as a parabola, where the lowest point is the best cost for the model (see Figure 2 below). These are the steps the model will take:\n",
    "\n",
    "1. Make a prediction.\n",
    "2. Update weights and biases.\n",
    "3. Make a new prediction.\n",
    "4. Compare old and new predictions.\n",
    "\n",
    "But, can we update the weights and biases in an intelligent way, instead of making random guesses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81686683",
   "metadata": {},
   "source": [
    "<img src=\"https://lucidar.me/en/neural-networks/files/gradient-overview.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<div style=\"width: 1000px; margin: 2px auto\">\n",
    "<span>Figure 2: Gradient Overview</span>\n",
    "<br>\n",
    "<span style='font-size: 10px;'>Image from <a href=\"https://lucidar.me/en/neural-networks/single-layer-gradient-descent/\">Gradient descent for neural networks</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64c38d",
   "metadata": {},
   "source": [
    "### Back Propagation\n",
    "\n",
    "If you look at two points on the parabola above, we can draw a line between them. This line has some slope, or gradient. The gradient points towards the optimal model fit at the bottom of the parabola. If we can compute this gradient, then we have an intelligent way to tell the model how to update the weights and biases. And, it turns out we can... using calculus! The derivative in calculus is a measure of the gradient. After completing a forward pass, we can go backwards through the network, calculating the derivatives at each layer. This will give us a measure of the gradients for each layer. Using the gradient, we can update the weights and biases in a smart way, allowing the model to learn.\n",
    "\n",
    "The calculus involved is beyond the scope of this tutuorial. Below I will provide the derivative functions for each activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34724a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_sigmoid(dA, Z):\n",
    "    sigmoid = sigmoid_activation(Z)\n",
    "    dZ = dA * sigmoid * (1.0 - sigmoid)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea75b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_softmax(dA, Z):\n",
    "    softmax = softmax_activation(Z)\n",
    "    dZ = dA * softmax * (1.0 - softmax)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_relu(dA, Z):\n",
    "    dZ = np.copy(dA)\n",
    "    dZ[Z <= 0.0] = 0.0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472249e",
   "metadata": {},
   "source": [
    "#### Single Backward Pass\n",
    "In the next function, I calculate the derivatives for a single layer, `dZ`. Then, I compute the derivatives for the weights (`dW`) and biases (`db`). And, finally, the derivative of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ_map = {\n",
    "    'sigmoid': dZ_sigmoid,\n",
    "    'relu': dZ_relu,\n",
    "    'softmax': dZ_softmax\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae3254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backward_pass(dA, W, b, Z, A_previous, activation):\n",
    "    \n",
    "    try:\n",
    "        backprop_activation = dZ_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The backprop activation {activation} is not recognized.\\nIt must be one of the following: {list(dZ_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    dZ = backprop_activation(dA, Z)\n",
    "    \n",
    "    dW = np.dot(dZ, np.transpose(A_previous)) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_previous = np.dot(np.transpose(dW), dZ)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b45c03",
   "metadata": {},
   "source": [
    "#### Full Backward Pass\n",
    "Each single backward pass will be combined as we step backwards through each layer in the neural network. After each step, the gradients for the weights and biases will be stored. These will be used to update the model in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1384f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_pass(y_pred, y, cache, network):\n",
    "    \n",
    "    stored_grads = {}\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    dA_previous = y_pred - np.transpose(y)\n",
    "    \n",
    "    for layer in reversed(range(1, len(network) + 1)):\n",
    "        activation = network[f'layer_{layer}']['activation']\n",
    "        layer_previous = layer - 1\n",
    "        \n",
    "        dA = dA_previous\n",
    "        \n",
    "        A_previous = cache[f'A_{layer_previous}']\n",
    "        Z = cache[f'Z_{layer}']\n",
    "        W = network[f'layer_{layer}']['W']\n",
    "        b = network[f'layer_{layer}']['b']\n",
    "        \n",
    "        dA_previous, dW, db = single_backward_pass(dA, W, b, Z, A_previous, activation)\n",
    "        stored_grads[f'dW_{layer}'] = dW\n",
    "        stored_grads[f'db_{layer}'] = db\n",
    "        \n",
    "    return stored_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960db053",
   "metadata": {},
   "source": [
    "### Update the Model\n",
    "In this function, I update the current weights and biases with new weights and biases for each layer using the gradients that were calculated during back propogation. This function takes an additional argument, called the learning rate. As you can see, the gradients are multiplied by the learning rate. Therefore, this parameter determines how fast the model can update its parameters. The learning rate basically controls the step size, as shown in the image above. If the learning rate is large, the model will take large steps as it tries to find the optimal fit. If the learning rate is small, the model will take small steps in the direction of the optimal fit. If the learning rate it too large, the model could bounce back and forth across the optimal fit without finding the best set of values. But, if the learning rate is too small, it could take the model forever to find the best fit. Therefore, the learning rate needs to be iteratively adjusted to find the best value for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(network, stored_grads, learning_rate):\n",
    "    for layer in range(1, len(network) + 1):\n",
    "        network[f'layer_{layer}']['W'] = network[f'layer_{layer}']['W'] - learning_rate * stored_grads[f'dW_{layer}']\n",
    "        network[f'layer_{layer}']['b'] = network[f'layer_{layer}']['b'] - learning_rate * stored_grads[f'db_{layer}']\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed41d30",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Now that we have created all the pieces for our neural network, we can put them together in a function to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d309ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y, network):\n",
    "    \n",
    "    stored_cost = []\n",
    "    \n",
    "    for epoch in range(HYPER_PARAMS['epochs']):\n",
    "        y_pred, cache = full_forward_pass(X, network)\n",
    "        cost = compute_cross_entropy_cost(y_pred, y)\n",
    "        if epoch == 0:\n",
    "            print(f' * The initial cost is {cost:0.3f}.')\n",
    "        stored_cost.append(cost)\n",
    "        stored_grads = full_backward_pass(y_pred, y, cache, network)\n",
    "        network = update_network(network, stored_grads, HYPER_PARAMS['learning_rate'])\n",
    "    final_accuracy = compute_accuracy(y_pred, y)\n",
    "    print(f' * Final cost: {cost:0.3f}.')\n",
    "    print(f' * Final accuracy: {final_accuracy:0.3%}')\n",
    "    return network, stored_cost, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2e4a2",
   "metadata": {},
   "source": [
    "But, before actually training the model, there are a few things we need to do to the dataset.\n",
    "\n",
    "## Data Preprocessing\n",
    "### Load Dataset\n",
    "We will load the dataset into a Pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4cad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_values = []\n",
    "with open('./data/zoo.dat', 'r') as zoo_file:\n",
    "    for line in zoo_file:\n",
    "        if '@attribute' in line:\n",
    "            header_values.append(line.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dcd021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/zoo.dat', skiprows=21, header=None, names=header_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d092e",
   "metadata": {},
   "source": [
    "### Separate Features and Labels\n",
    "Next, we need to separate the features and labels. Each feature is some characteristic of an animal, as shown above. The labels are in the column _Type_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.drop(columns='Type')  # Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455265ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df['Type']  # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6ee6b",
   "metadata": {},
   "source": [
    "### One-hot Encode Features\n",
    "When training a machine learning model, it will perform better if all features are scaled to the same range of values. In looking at the table above you will notice that all features except _Legs_ have values that are either one or zero. In addition, _Legs_ is a categorical variable and so we can't scale it using a numerical transformation. Instead, we can encode each label as a vector of 1's and 0's, which is called one-hot encoding.\n",
    "\n",
    "#### What is one-hot encoding?\n",
    "\n",
    "Let's say we are categorizing fruit and we have the following categories:\n",
    "\n",
    "`['apples', 'bananas', 'pears', 'peaches']`\n",
    "\n",
    "Let's say we have the label 'bananas', we can encode it using one's and zero's using the catergory list above. We will place a 1 for the index in the list that corresponds to 'bananas' and 0 elsewhere. Therefore, the one-hot encoded vector for bananas will be:\n",
    "\n",
    "`[0, 1, 0, 0]`\n",
    "\n",
    "We can reduce the length of the one-hot vector, but retain the same information, by dropping one of the categories in the category list:\n",
    "\n",
    "`['apples', 'bananas', 'pears']`\n",
    "\n",
    "In this case, the label 'peaches' would correspond to the following vector:\n",
    "\n",
    "`[0, 0, 0]`\n",
    "\n",
    "One-hot encoding in Python can be done using Pandas `get_dummies()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_one_hot = pd.get_dummies(df_X, columns=['Legs'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_one_hot.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92502392",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X_one_hot.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f76e12",
   "metadata": {},
   "source": [
    "### One-hot Encode Labels\n",
    "We will also one-hot encode the labels, since these are categorical variables. Furthermore, since we are using softmax to make predictions, which encodes the labels as vectors of class probability, we need the true labels to also be vectors. This will allow us to evaluate the model in the way we discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8634ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(pd.unique(df_y))\n",
    "print(f'There are {NUM_CLASSES} unique classes for the labels, which are {pd.unique(df_y)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf931cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(x):\n",
    "    encoded = np.zeros(NUM_CLASSES)\n",
    "    encoded[x-1] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb198d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_one_hot = df_y.apply(lambda x: encode_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_one_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0021311",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_y_one_hot.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c280aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d574dd",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "At this point the data has been prepared for modeling. During modeling, we are going to define two hyperparameters. Hyperparameters are parameters that can be chosen to adjust how the model trains. In this case, we will use _epochs_ and _learning_rate_. The learning rate was discussed early. Epochs controls how many times the model will update its parameters in search of the best model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Adjust these hyperparameter values and see how the model performance changes.\n",
    "HYPER_PARAMS = {\n",
    "    'epochs': 10000,\n",
    "    'learning_rate': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391b6c9",
   "metadata": {},
   "source": [
    "Let's run the model and evaluate it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "network, stored_cost, y_pred = train_nn(X, y, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd824c0",
   "metadata": {},
   "source": [
    "Wow! While the cost is small, the accuracy really isn't good! In order to understand why this might be, let's take a look at the distribution in different classes for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label_counts(y):\n",
    "    y_label_summary = pd.Series(y).value_counts(normalize=True).reset_index().sort_values(by='index')\n",
    "    y_label_summary.columns = ['Label', 'Fraction']\n",
    "    return y_label_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05991d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_counts = compute_label_counts(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6acf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761dc4b",
   "metadata": {},
   "source": [
    "You will notice that 40% of the values are of class 1, 20% are of class 2, 13% are of class 4, while the rest of the classes are much smaller. This unequal distribution in classes is called class imbalance.\n",
    "\n",
    "Let's take a look at the distribution in classes for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a082bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_label_predictions(y_pred):\n",
    "    y_pred_transpose = np.transpose(y_pred)\n",
    "    y_pred_flat = np.argmax(y_pred_transpose, 1)\n",
    "    return y_pred_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc759f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_flat = flatten_label_predictions(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_flat += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e706b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_counts = compute_label_counts(y_pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa446e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ed698",
   "metadata": {},
   "source": [
    "Over 80% of the predictions went to class 1. Basically, since class 1 dominates the training set, the model is deciding to classify almost all samples as this class. Therefore, something needs to be done to counter this class imbalance and help the model perform better.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "* **1:** Play with the network architecture (number of layers, number of nodes per layer, activation functions) to see if you can improve the model performance.\n",
    "* **2:** Similarly, try improving the model performance by adjusting the hyperparameters.\n",
    "\n",
    "_**Take Home Challenge:**_\n",
    "* **3:** Create a model using numpy that handles the class imbalance. Hint: Trying modifying the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35eb40",
   "metadata": {},
   "source": [
    "### Resources\n",
    "This notebook has been inspired by the Towards Data Science post [Letâ€™s code a Neural Network in plain NumPy](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795).\n",
    "\n",
    "Additional resources include:\n",
    "\n",
    "* [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/).\n",
    "* [Creating a Neural Network from Scratch in Python: Multi-class Classification](https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-multi-class-classification/).\n",
    "* [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/).\n",
    "* [Understanding and implementing Neural Network with SoftMax in Python from scratch](http://www.adeveloperdiary.com/data-science/deep-learning/neural-network-with-softmax-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee16c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
