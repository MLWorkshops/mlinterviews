{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subtle-disposition",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial: An Introduction to Neural Networks\n",
    "\n",
    "In this tutorial you are going to learn the basics of neural networks. We will walk through each of the different parts that make up a neural network. In the end, we will put these parts together to create a model that can classify animals from a zoo dataset.\n",
    "\n",
    "The model will be developed from first principles using `Numpy`. This means we can see the inner workers of the neural network and how each part works together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mounted-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-uruguay",
   "metadata": {},
   "source": [
    "## What is a neural network?\n",
    "So, what is a neural network. A neural network can be thought of as a more complicated linear equation.\n",
    "\n",
    "$y = m*x + b$\n",
    "\n",
    "Given some input (`x`), the model applies some weights (`m`) and biases (`b`) to predict an outcome, `y`.\n",
    "\n",
    "But, why is it called 'neural'? That seems to imply something related to the brain. For that, we need to introduce the perceptron.\n",
    "\n",
    "## What is a perceptron?\n",
    "The perceptron is the most basic unit of a neural network. It functions much like a neuron in our brain. Each neuron in our brain reserves signals from dendrites. Depending on the signals received, the neuron will fire or remain quite.\n",
    "\n",
    "The perceptron functions in much the same way. Each perceptron receives inputs from adjoining perceptrons. It will then combine these inputs and output either zero or a non-zero value. The functions that are used to make these decisions are called activation functions. Let's take a look at a couple of these activation functions.\n",
    "\n",
    "### Sigmoid Activation\n",
    "This activation function sets all inputs to values between 0 and 1. It does this using an exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorporated-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(Z):\n",
    "    activation = 1/(1 + np.exp(-1*Z))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-reverse",
   "metadata": {},
   "source": [
    "### Relu Activation\n",
    "This activation function sets all negative values to 0 and otherwise returns the positive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cooperative-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(Z):\n",
    "    activation = np.maximum(0.0, Z)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-inside",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "In our model, we are going to perform classification. Therefore, the neural network needs a way to predict a class as output, given some input features. This can be achieved by using a softmax function, which assigns a probability to each class. All probabibilities add up to one. The class with the highest probability is assigned the prediction for that class. The hidden layers allow the neural network to learn complex relationships between the input features to help it make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "considerable-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z))\n",
    "    activation = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-accommodation",
   "metadata": {},
   "source": [
    "### Connecting Perceptrons\n",
    "Now that we have the 'neurons' (perceptrons) of the neural network, we need to create the network by connecting perceptrons together. Neurons are aligned in layers. The features are received as inputs to the initial layer. The final layer returns a prediction for the model. Inbetween are hidden layers. They are called hidden because we do not have access to information passed to these neurons. Let's define the architecture that we will use for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fresh-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 20\n",
    "NUM_CLASSES = 7\n",
    "LAYER_SIZES = [25, 25]\n",
    "\n",
    "LAYER_SIZES.insert(0, NUM_FEATURES)\n",
    "\n",
    "LAYER_SIZES.append(NUM_CLASSES)\n",
    "\n",
    "LAYER_ACTIVATIONS = ['relu', 'relu', 'softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "swedish-cabin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer sizes include: [20, 25, 25, 7].\n"
     ]
    }
   ],
   "source": [
    "print(f'The layer sizes include: {LAYER_SIZES}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-bottom",
   "metadata": {},
   "source": [
    "Out dataset contains 20 features, so that is the input. The first and second layers contain 25 neurons. The final layer is the softmax layer, with seven outputs, one of each type of animal being predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-scottish",
   "metadata": {},
   "source": [
    "### How are values passed between layers?\n",
    "\n",
    "Mathematically, this is done by using a weighted sum function. For each layer, the inputs to the perceptrons are multiplied by a weight. The weighted inputs are then added together. Finally, a bias term is added to the sum. This can be done exhaustively. But, a more elagant way is using linear algebra. Here is an example using numpy:\n",
    "\n",
    "`Z = np.dot(np.transpose(W), X) + b`\n",
    "\n",
    "`Z` is now a vector of size equal to the number of neurons in the layer. The vector `Z` now passes through an activation function before being passed to the next layer in the neural network.\n",
    "\n",
    "### Layer Initialization\n",
    "One of the most import steps in setting up a neural network is initiating the weights and biases. Basically, the neural network needs a starting point at which to begin it's learning process. The choice of initial values is very important. If all values are the same, then the outputs to each hidden layer will be the same. This will prevent each neuron in the hidden layer from learning anything useful. To prevent this problem, the weights and biases are initialized to small values. Let's initialize our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instructional-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network():\n",
    "    architecture = {}\n",
    "    for layer in range(1, len(LAYER_SIZES)):\n",
    "        architecture[f'layer_{layer}'] = {\n",
    "            'W': np.random.randn(LAYER_SIZES[layer],\n",
    "                                 LAYER_SIZES[layer-1]) * 0.1,\n",
    "            'b': np.random.randn(LAYER_SIZES[layer], 1) * 0.1,\n",
    "            'activation': LAYER_ACTIVATIONS[layer-1]\n",
    "        }\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unexpected-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = initialize_network()\n",
    "# pprint.pprint(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-reproduction",
   "metadata": {},
   "source": [
    "### Single Forward Pass\n",
    "Ok, now we have defined our neural network architecture and have initialized all weights and biases. We now need to define a function that will pass values from layer to layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "coastal-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_map = {\n",
    "    'sigmoid': sigmoid_activation,\n",
    "    'relu': relu_activation,\n",
    "    'softmax': softmax_activation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fabulous-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_forward_pass(A_previous, W, b, activation):\n",
    "    try:\n",
    "        act_function = act_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The activation {activation} is not recognized.\\nIt must be one of the following: {list(act_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    Z = np.dot(W, A_previous) + b\n",
    "    A = act_function(Z)\n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-perth",
   "metadata": {},
   "source": [
    "This function using linear algebra to perform the weighted sum for each layer that was discussed above. It outputs the activated outputs `A` and the non-activated outputs `Z`.\n",
    "\n",
    "### Full Foward Pass\n",
    "In the next function I define, we loop through each layer in the network and perform a forward pass using `single_forward_pass()`. All activated outputs `A` and non-activate outputs `Z` will be store for later use. We will get to that next. After the inputs have been passed in, the final output will be the predictions for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "educational-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_pass(X, network):\n",
    "    \n",
    "    cache = {}\n",
    "    A = np.transpose(X)\n",
    "    \n",
    "    for layer in range(1, len(network) + 1):\n",
    "        A_previous = A\n",
    "        A, Z = single_forward_pass(A_previous,\n",
    "                                   network[f'layer_{layer}']['W'], \n",
    "                                   network[f'layer_{layer}']['b'], \n",
    "                                   network[f'layer_{layer}']['activation'])\n",
    "        \n",
    "        cache[f'A_{layer-1}'] = A_previous\n",
    "        cache[f'Z_{layer}'] = Z\n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-newcastle",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "After making predictions, we need a way to evaluate the model. We would like to know how well the model can predict each label. The simplest method is to compute the accuracy. For each prediction, compare it to the true value. Then count up the fraction of correct predictions. Here is a function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parental-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    y_pred_transpose = np.transpose(y_pred)\n",
    "    y_pred_flat = np.argmax(y_pred_transpose, 1)\n",
    "    y_flat = np.argmax(y, 1)\n",
    "    accuracy = np.mean(y_pred_flat == y_flat)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-springer",
   "metadata": {},
   "source": [
    "You will notice that periodically I will using `np.transpose()`. This will ensure that when comparing predictions and labels, each will have the same shape.\n",
    "\n",
    "### Cross Entropy Cost Function\n",
    "While accuracy can provide a quick way to assess model performance, it does not provide much information on how well the model does at predicting each of the different classes. As shown above, the output of the softmax function will produce probabilities for each class for each sample in the training set. We can compare the probabilities to the actual values using a cross entropy function. Here is the formula:\n",
    "\n",
    "$cost = -1 * \\sum \\limits _{i} ^m y_{i} * log(y'_{i})$\n",
    "\n",
    "Where $y$ is the true value and $y'$ is the prediction.\n",
    "\n",
    "Here is how we can code up cross entropy in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "objective-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_cost(y_pred, y):\n",
    "    \n",
    "    cost = -1*np.mean(y * np.log(np.transpose(y_pred)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-afghanistan",
   "metadata": {},
   "source": [
    "Great! Now we have a way to evaluate how well out model is at classification. But, out model will be useless if it can learn anything. We need it to be able to decide how to update it's weights and biases in order to improve it's ability to make predictions.\n",
    "\n",
    "### Gradients\n",
    "During training, we want our model to search for the best fit. This will be the combination of parameters that yields the smallest cost function. In simplest terms, this search space can be thought of as a parabola, where the lowest point in the best cost for the model (see image below). These are the steps the model will take:\n",
    "\n",
    "1. Make a prediction.\n",
    "2. Update weights and biases.\n",
    "3. Make a new prediction.\n",
    "4. Compare old and new predictions.\n",
    "\n",
    "But, can we update the weights and biases in an intelligent way, instead of making random guesses?\n",
    "\n",
    "### Back Propagation\n",
    "\n",
    "If you look at the two points on the parabola above, we can draw a line between them. This line has some slope, or gradient. The gradient points towards the optimal model fit at the bottom of the parabola. If we can compute this gradient, then we have an intelligent way to tell the model how to update the weights and biases. And, it turns out we can... using calculus! The derivative in calculus is a measure of the gradient. After completing a forward pass, we can go backwards through the network, calculating the derivatives at each layer. This will give us a measure of the gradient. Using the gradient, we can update the weights and biases in a smart way, allowing the model to learn.\n",
    "\n",
    "The calculus involved is beyond the scope of this tutuorial. Below I will provide the derivative functions for each activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opponent-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_sigmoid(dA, Z):\n",
    "    sigmoid = sigmoid_activation(Z)\n",
    "    dZ = dA * sigmoid * (1.0 - sigmoid)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "leading-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_softmax(dA, Z):\n",
    "    softmax = softmax_activation(Z)\n",
    "    dZ = dA * softmax * (1.0 - softmax)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "thorough-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_relu(dA, Z):\n",
    "    dZ = np.copy(dA)\n",
    "    dZ[Z <= 0.0] = 0.0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-amsterdam",
   "metadata": {},
   "source": [
    "#### Single Backward Pass\n",
    "In the next function, I calculate the derivatives for a single layer, `dZ`. Then, I compute the derivatives for the weights (`dW`) and biases (`db`). And, finally, the derivative of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abandoned-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ_map = {\n",
    "    'sigmoid': dZ_sigmoid,\n",
    "    'relu': dZ_relu,\n",
    "    'softmax': dZ_softmax\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vietnamese-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backward_pass(dA, W, b, Z, A_previous, activation):\n",
    "    \n",
    "    try:\n",
    "        backprop_activation = dZ_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The backprop activation {activation} is not recognized.\\nIt must be one of the following: {list(dZ_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    dZ = backprop_activation(dA, Z)\n",
    "    \n",
    "    dW = np.dot(dZ, np.transpose(A_previous)) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_previous = np.dot(np.transpose(dW), dZ)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-identity",
   "metadata": {},
   "source": [
    "#### Full Backward Pass\n",
    "Each single backward pass will be combined as we step backwards through each layer in the neural network. After each step, the gradients for the weights and biases will be stored. These will be used to update the model in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "quality-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_pass(y_pred, y, cache, network):\n",
    "    \n",
    "    stored_grads = {}\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    dA_previous = y_pred - np.transpose(y)\n",
    "    \n",
    "    for layer in reversed(range(1, len(network) + 1)):\n",
    "        activation = network[f'layer_{layer}']['activation']\n",
    "        layer_previous = layer - 1\n",
    "        \n",
    "        dA = dA_previous\n",
    "        \n",
    "        A_previous = cache[f'A_{layer_previous}']\n",
    "        Z = cache[f'Z_{layer}']\n",
    "        W = network[f'layer_{layer}']['W']\n",
    "        b = network[f'layer_{layer}']['b']\n",
    "        \n",
    "        dA_previous, dW, db = single_backward_pass(dA, W, b, Z, A_previous, activation)\n",
    "        stored_grads[f'dW_{layer}'] = dW\n",
    "        stored_grads[f'db_{layer}'] = db\n",
    "        \n",
    "    return stored_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-finger",
   "metadata": {},
   "source": [
    "### Update the Model\n",
    "In this function, I update the current weights and biases with new weights and biases for each layer using the gradients that were calculated during back propogation. This function takes an additional argument, called the learning rate. As you can see, the gradients are multiplied the the learning rate. Therefore, this parameter determines how fast the model can update its parameters. If the learning rate basically controls the step size, as shown in the image above. If the learning rate is large, the model will take large steps as it tries to find the optimal fit. If the learning rate is small, the model will take small steps in the direction of the optimal fit. If the learning rate it too large, the model could bounce back and forth across the optimal fit without finding the best set of values. But, if the learning rate is too small, it could take the model forever to find the best fit. Therefore, the learning rate needs to be iteratively adjusted to find the best value for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fantastic-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(network, stored_grads, learning_rate):\n",
    "    for layer in range(1, len(network) + 1):\n",
    "        network[f'layer_{layer}']['W'] = network[f'layer_{layer}']['W'] - learning_rate * stored_grads[f'dW_{layer}']\n",
    "        network[f'layer_{layer}']['b'] = network[f'layer_{layer}']['b'] - learning_rate * stored_grads[f'db_{layer}']\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-friday",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Now that we have created all the pieces for our neural network, we can put them together in a function to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "interpreted-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y, network):\n",
    "    \n",
    "    stored_cost = []\n",
    "    \n",
    "    for epoch in range(HYPER_PARAMS['epochs']):\n",
    "        y_pred, cache = full_forward_pass(X, network)\n",
    "        cost = compute_cross_entropy_cost(y_pred, y)\n",
    "        if epoch == 0:\n",
    "            print(f' * The initial cost is {cost:0.3f}.')\n",
    "        stored_cost.append(cost)\n",
    "        stored_grads = full_backward_pass(y_pred, y, cache, network)\n",
    "        network = update_network(network, stored_grads, HYPER_PARAMS['learning_rate'])\n",
    "    final_accuracy = compute_accuracy(y_pred, y)\n",
    "    print(f' * Final cost: {cost:0.3f}.')\n",
    "    print(f' * Final accuracy: {final_accuracy:0.3%}')\n",
    "    return network, stored_cost, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-exercise",
   "metadata": {},
   "source": [
    "But, before actually training the model, there are a few things we need to do to the dataset.\n",
    "\n",
    "## Data Preprocessing\n",
    "### Load Dataset\n",
    "We will load the dataset into a Pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "selective-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_values = []\n",
    "with open('./data/zoo.dat', 'r') as zoo_file:\n",
    "    for line in zoo_file:\n",
    "        if '@attribute' in line:\n",
    "            header_values.append(line.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "respective-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/zoo.dat', skiprows=21, header=None, names=header_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "conservative-enemy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hair</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.425743</td>\n",
       "      <td>0.496921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feathers</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.400495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eggs</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Milk</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>0.493522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airborne</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.427750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aquatic</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.481335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predator</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.499505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toothed</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>0.491512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Backbone</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.384605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breathes</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.407844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Venomous</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.271410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fins</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.376013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Legs</th>\n",
       "      <td>101.0</td>\n",
       "      <td>2.841584</td>\n",
       "      <td>2.033385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tail</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0.439397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domestic</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.336552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catsize</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>0.498314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>101.0</td>\n",
       "      <td>2.831683</td>\n",
       "      <td>2.102709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      mean       std  min  25%  50%  75%  max\n",
       "Hair      101.0  0.425743  0.496921  0.0  0.0  0.0  1.0  1.0\n",
       "Feathers  101.0  0.198020  0.400495  0.0  0.0  0.0  0.0  1.0\n",
       "Eggs      101.0  0.584158  0.495325  0.0  0.0  1.0  1.0  1.0\n",
       "Milk      101.0  0.405941  0.493522  0.0  0.0  0.0  1.0  1.0\n",
       "Airborne  101.0  0.237624  0.427750  0.0  0.0  0.0  0.0  1.0\n",
       "Aquatic   101.0  0.356436  0.481335  0.0  0.0  0.0  1.0  1.0\n",
       "Predator  101.0  0.554455  0.499505  0.0  0.0  1.0  1.0  1.0\n",
       "Toothed   101.0  0.603960  0.491512  0.0  0.0  1.0  1.0  1.0\n",
       "Backbone  101.0  0.821782  0.384605  0.0  1.0  1.0  1.0  1.0\n",
       "Breathes  101.0  0.792079  0.407844  0.0  1.0  1.0  1.0  1.0\n",
       "Venomous  101.0  0.079208  0.271410  0.0  0.0  0.0  0.0  1.0\n",
       "Fins      101.0  0.168317  0.376013  0.0  0.0  0.0  0.0  1.0\n",
       "Legs      101.0  2.841584  2.033385  0.0  2.0  4.0  4.0  8.0\n",
       "Tail      101.0  0.742574  0.439397  0.0  0.0  1.0  1.0  1.0\n",
       "Domestic  101.0  0.128713  0.336552  0.0  0.0  0.0  0.0  1.0\n",
       "Catsize   101.0  0.435644  0.498314  0.0  0.0  0.0  1.0  1.0\n",
       "Type      101.0  2.831683  2.102709  1.0  1.0  2.0  4.0  7.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-wallpaper",
   "metadata": {},
   "source": [
    "### Separate Features and Labels\n",
    "Next, we need to separate the features and labels. Each feature is some characteristic of an an animal, as shown above. The labels are in column _Type_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "configured-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.drop(columns='Type')  # Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adolescent-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df['Type']  # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-intro",
   "metadata": {},
   "source": [
    "### One-hot Encode Features\n",
    "When training a machine learning model, it will perform better if all features are scaled to the same range of values. In looking at the table above you will notice that all features except _Legs_ are have values that are either one or zero. In addition, _Legs_ is a categorical variable and so we can't scale it using a numerical transformation. Instead, we can encode each label as a vector of 1's and 0's. \n",
    "\n",
    "#### What is one-hot encoding?\n",
    "\n",
    "Let's say we are categorizing fruit and we have the following categories:\n",
    "\n",
    "`['apples', 'bananas', 'pears', 'peaches']`\n",
    "\n",
    "Let's say we have the label 'bananas', we can encode it using one's and zero's using the catergory list above. We will place a 1 for the index in the list that corresponds to 'bananas' and 0 elsewhere. Therefore, the one-hot encoded vector for bananas will be:\n",
    "\n",
    "`[0, 1, 0, 0]`\n",
    "\n",
    "We can reduce the length of the one-hot vector, but retain the same information, but dropping one of the categories in the category list:\n",
    "\n",
    "`['apples', 'bananas', 'pears']`\n",
    "\n",
    "In this case, the label 'peaches' would correspond to the following vector:\n",
    "\n",
    "`[0, 0, 0]`\n",
    "\n",
    "One-hot encoding in Python can be done using Pandas `get_dummies()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "textile-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_one_hot = pd.get_dummies(df_X, columns=['Legs'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sapphire-brave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hair</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.425743</td>\n",
       "      <td>0.496921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feathers</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.400495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eggs</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Milk</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>0.493522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airborne</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.427750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aquatic</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.481335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predator</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.499505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toothed</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>0.491512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Backbone</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.384605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breathes</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.407844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Venomous</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.271410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fins</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.376013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tail</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0.439397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domestic</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.336552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catsize</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>0.498314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Legs_2</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.444772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Legs_4</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.486857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Legs_5</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.099504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Legs_6</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.300165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Legs_8</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.140014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      mean       std  min  25%  50%  75%  max\n",
       "Hair      101.0  0.425743  0.496921  0.0  0.0  0.0  1.0  1.0\n",
       "Feathers  101.0  0.198020  0.400495  0.0  0.0  0.0  0.0  1.0\n",
       "Eggs      101.0  0.584158  0.495325  0.0  0.0  1.0  1.0  1.0\n",
       "Milk      101.0  0.405941  0.493522  0.0  0.0  0.0  1.0  1.0\n",
       "Airborne  101.0  0.237624  0.427750  0.0  0.0  0.0  0.0  1.0\n",
       "Aquatic   101.0  0.356436  0.481335  0.0  0.0  0.0  1.0  1.0\n",
       "Predator  101.0  0.554455  0.499505  0.0  0.0  1.0  1.0  1.0\n",
       "Toothed   101.0  0.603960  0.491512  0.0  0.0  1.0  1.0  1.0\n",
       "Backbone  101.0  0.821782  0.384605  0.0  1.0  1.0  1.0  1.0\n",
       "Breathes  101.0  0.792079  0.407844  0.0  1.0  1.0  1.0  1.0\n",
       "Venomous  101.0  0.079208  0.271410  0.0  0.0  0.0  0.0  1.0\n",
       "Fins      101.0  0.168317  0.376013  0.0  0.0  0.0  0.0  1.0\n",
       "Tail      101.0  0.742574  0.439397  0.0  0.0  1.0  1.0  1.0\n",
       "Domestic  101.0  0.128713  0.336552  0.0  0.0  0.0  0.0  1.0\n",
       "Catsize   101.0  0.435644  0.498314  0.0  0.0  0.0  1.0  1.0\n",
       "Legs_2    101.0  0.267327  0.444772  0.0  0.0  0.0  1.0  1.0\n",
       "Legs_4    101.0  0.376238  0.486857  0.0  0.0  0.0  1.0  1.0\n",
       "Legs_5    101.0  0.009901  0.099504  0.0  0.0  0.0  0.0  1.0\n",
       "Legs_6    101.0  0.099010  0.300165  0.0  0.0  0.0  0.0  1.0\n",
       "Legs_8    101.0  0.019802  0.140014  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_one_hot.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "chemical-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X_one_hot.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-cooling",
   "metadata": {},
   "source": [
    "### One-hot Encode Labels\n",
    "We will also one-hot encode the labels, since these are categorical variables. Furthermore, since we are using softmax to make predictions, which encodes the labels as vectors of class probability, we need the true labels to also be vectors. This will allow us to evaluate the model as we discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "illegal-correspondence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 unique classes for the labels, which are [1 4 7 2 6 3 5].\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(pd.unique(df_y))\n",
    "print(f'There are {NUM_CLASSES} unique classes for the labels, which are {pd.unique(df_y)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "through-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(x):\n",
    "    encoded = np.zeros(NUM_CLASSES)\n",
    "    encoded[x-1] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "criminal-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_one_hot = df_y.apply(lambda x: encode_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "critical-wheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "1    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "2    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
       "Name: Type, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_one_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "differential-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_y_one_hot.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "excellent-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-speaking",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "At this point the data has been prepared for model. During model, we are going to define two hyperparameters. Hyperparameters are parameters that can be chosen to adjust how the model trains. In this case, we will use _epochs_ and _learning_rate_. The learning rate was discussed early. Epochs controls how many times the model will update its parameters in search of the best model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ahead-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {\n",
    "    'epochs': 10000,\n",
    "    'learning_rate': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-classification",
   "metadata": {},
   "source": [
    "Let's run the model and evaluate it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "charitable-flush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * The initial cost is 0.285.\n",
      " * Final cost: 0.348.\n",
      " * Final accuracy: 40.594%\n"
     ]
    }
   ],
   "source": [
    "network, stored_cost, y_pred = train_nn(X, y, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-adrian",
   "metadata": {},
   "source": [
    "Wow! While the cost is small, the accuracy really isn't good! In order to understand why this might be, let's take a look at the distribution in different classes for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "further-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_label_counts(y):\n",
    "    y_label_summary = pd.Series(y).value_counts(normalize=True).reset_index().sort_values(by='index')\n",
    "    y_label_summary.columns = ['Label', 'Fraction']\n",
    "    return y_label_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "intermediate-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_counts = compute_label_counts(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "equipped-husband",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.405941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.198020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.128713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.039604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.079208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.099010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Fraction\n",
       "0      1  0.405941\n",
       "1      2  0.198020\n",
       "5      3  0.049505\n",
       "2      4  0.128713\n",
       "6      5  0.039604\n",
       "4      6  0.079208\n",
       "3      7  0.099010"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-dimension",
   "metadata": {},
   "source": [
    "You will notice that 40% of the values are of class 1, 20% are of class 2, 13% are of class 4, while the rest of the classes are much smaller. This unequal distribution in classes is called class imbalance.\n",
    "\n",
    "Let's take a look at the distribution in classes for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "greatest-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_label_predictions(y_pred):\n",
    "    y_pred_transpose = np.transpose(y_pred)\n",
    "    y_pred_flat = np.argmax(y_pred_transpose, 1)\n",
    "    return y_pred_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "prime-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_flat = flatten_label_predictions(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "requested-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_flat += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "physical-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_counts = compute_label_counts(y_pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "played-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.841584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.019802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.118812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Fraction\n",
       "0      1  0.841584\n",
       "2      2  0.019802\n",
       "3      3  0.009901\n",
       "1      4  0.118812\n",
       "4      7  0.009901"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-wales",
   "metadata": {},
   "source": [
    "Over 80% of the predictions went to class 1. Basically, since class 1 dominates the training set, the model is deciding to classify almost all samples as this class. Therefore, something needs to be done to counter this class imbalance and help the model perform better.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "* **1:** Create a model using numpy that handles the class imbalance.\n",
    "* **2:** Play with the network architecture (number of layers, number of node per layer, activation functions) to see if you can improve the model performance.\n",
    "* **3:** Similarly, try improving the model performance by adjusting the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-ballot",
   "metadata": {},
   "source": [
    "### Resources\n",
    "This notebook has been inspired by the Towards Data Science post [Let’s code a Neural Network in plain NumPy](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795).\n",
    "\n",
    "Additional resources include:\n",
    "\n",
    "* [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/).\n",
    "* [Creating a Neural Network from Scratch in Python: Multi-class Classification](https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-multi-class-classification/).\n",
    "* [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/).\n",
    "* [Understanding and implementing Neural Network with SoftMax in Python from scratch](http://www.adeveloperdiary.com/data-science/deep-learning/neural-network-with-softmax-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-kingston",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
