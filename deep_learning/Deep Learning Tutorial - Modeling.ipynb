{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "committed-project",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial - Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "first-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inappropriate-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 2\n",
    "NUM_CLASSES = 7\n",
    "LAYER_SIZES = [5, 5]\n",
    "\n",
    "LAYER_SIZES.insert(0, INPUT_SIZE)\n",
    "\n",
    "LAYER_SIZES.append(NUM_CLASSES)\n",
    "\n",
    "LAYER_ACTIVATIONS = ['relu', 'relu', 'softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "harmful-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer sizes include: [2, 5, 5, 7].\n"
     ]
    }
   ],
   "source": [
    "print(f'The layer sizes include: {LAYER_SIZES}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "public-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network():\n",
    "    architecture = {}\n",
    "    for layer in range(1, len(LAYER_SIZES)):\n",
    "        architecture[f'layer_{layer}'] = {\n",
    "            'w': np.random.randn(LAYER_SIZES[layer],\n",
    "                                 LAYER_SIZES[layer-1]) * 1,\n",
    "            'b': np.zeros(LAYER_SIZES[layer]),\n",
    "            'activation': LAYER_ACTIVATIONS[layer-1]\n",
    "        }\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empty-washer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_1': {'activation': 'relu',\n",
      "             'b': array([0., 0., 0., 0., 0.]),\n",
      "             'w': array([[ 0.6436408 ,  0.80006476],\n",
      "       [ 0.73563585,  1.27752534],\n",
      "       [ 0.09365557, -1.98953266],\n",
      "       [-0.03147536,  1.17531548],\n",
      "       [-1.21081442,  1.27439103]])},\n",
      " 'layer_2': {'activation': 'relu',\n",
      "             'b': array([0., 0., 0., 0., 0.]),\n",
      "             'w': array([[ 1.47388233,  1.01047831, -0.43945397, -0.03241503, -0.7398889 ],\n",
      "       [ 0.42247096,  0.73227551,  0.0689344 , -0.76239729,  0.49743781],\n",
      "       [-0.43257698, -0.52342636,  0.64388953,  1.20068519, -0.28892381],\n",
      "       [-0.07131636, -0.51847173, -1.12926377,  0.81151567, -1.44163607],\n",
      "       [ 1.11800781, -1.05299348, -0.27996439, -1.63622441,  0.15015977]])},\n",
      " 'layer_3': {'activation': 'softmax',\n",
      "             'b': array([0., 0., 0., 0., 0., 0., 0.]),\n",
      "             'w': array([[ 0.69802614, -1.76077029, -0.77230799, -1.28211465,  0.43704024],\n",
      "       [ 0.30109275,  0.67040528, -0.96376584, -0.33572579,  0.09996639],\n",
      "       [ 0.95630123, -0.00484139, -0.06171383,  0.46900009, -1.87853448],\n",
      "       [-0.66943019, -1.09653124, -0.21152282, -0.26774669, -0.82870341],\n",
      "       [ 0.12908138, -1.98781004,  0.18871418,  0.55526953, -0.34401101],\n",
      "       [ 1.33483979,  0.04577747,  0.35569524, -0.084032  ,  0.53730458],\n",
      "       [ 1.12390673, -0.35161912, -1.74404125, -1.00690907, -0.63862673]])}}\n"
     ]
    }
   ],
   "source": [
    "network = initialize_network()\n",
    "pprint.pprint(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "original-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(Z):\n",
    "    activation = 1/(1 + np.exp(-1*Z))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "related-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(Z):\n",
    "    activation = np.exp(Z) / np.sum(np.exp(Z))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dominant-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(Z):\n",
    "    activation = np.max(0, Z)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "christian-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_sigmoid(dA, Z):\n",
    "    sigmoid = sigmoid_activation(Z)\n",
    "    dZ = dA * sigmoid * (1 - sigmoid)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "plastic-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_softmax(Z):\n",
    "    softmax = softmax_activation(Z)\n",
    "    softmax_matrix = np.tile(softmax)\n",
    "    dZ = np.diag(softmax) - (softmax_matrix*np.transpose(softmax_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "linear-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_relu(dA, Z):\n",
    "    dZ = np.copy(dA)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "removable-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_map = {\n",
    "    'sigmoid': sigmoid_activation,\n",
    "    'relu': relu_activation,\n",
    "    'softmax': softmax_activation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "domestic-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ_map = {\n",
    "    'sigmoid': dZ_sigmoid,\n",
    "    'relu': dZ_relu,\n",
    "    'softmax': dZ_softmax\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "likely-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_forward_pass(A_previous, W, b, activation):\n",
    "    try:\n",
    "        act_function = act_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The activation {activation} is not recognized.\\nIt must be one of the following: {list(act_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    Z = np.dot(W, A_previous) + b\n",
    "    A = act_function(Z)\n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mediterranean-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_pass(X, network):\n",
    "    \n",
    "    cache = {}\n",
    "    A = X\n",
    "    \n",
    "    for layer in range(1, len(network) + 1):\n",
    "        \n",
    "        A_previous = A\n",
    "        A, Z = single_forward_pass(A_previous, network[layer]['W'], network[layer]['b'], network[layer]['activation'])\n",
    "        \n",
    "        cache[f'A_{layer-1}'] = A_previous\n",
    "        cache[f'Z_{layer}'] = Z\n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "electrical-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_cost(y_pred, y):\n",
    "    \n",
    "    cost = np.sum(-1*(y * np.log(y_pred)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "democratic-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backward_pass(dA, W, b, Z, A_previous, activation):\n",
    "    \n",
    "    try:\n",
    "        backprop_activation = dZ_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The backprop activation {activation} is not recognized.\\nIt must be one of the following: {list(dZ_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    if activations == 'softmax':\n",
    "        dZ = backprop_activation(Z)\n",
    "    else:\n",
    "        dZ = backprop_activation(dA, Z)\n",
    "    \n",
    "    dW = np.dot(dZ, np.transpose(A_previous)) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_previous = np.dot(np.transpose(dW), dZ)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suited-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_pass(y_pred, y, cache, network):\n",
    "    \n",
    "    stored_grads = {}\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    dA_previous = A - y\n",
    "    \n",
    "    for layer in reversed(range(1, len(network) + 1)):\n",
    "        activation = network[layer]['activation']\n",
    "        layer_previous = layer - 1\n",
    "        \n",
    "        dA = dA_previous\n",
    "        \n",
    "        A_previous = cache[f'A_{layer_previous}']\n",
    "        Z = cache[f'Z_{layer}']\n",
    "        W = network[layer]['W']\n",
    "        b = network[layer]['b']\n",
    "        \n",
    "        dA_previous, dW, db = single_backward_pass(dA, W, b, Z, A_previous, activation)\n",
    "        stored_grads[f'dW_{layer}'] = dW\n",
    "        stored_grads[f'db_{layer}'] = db\n",
    "        \n",
    "    return stored_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "humanitarian-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(network, stored_grads, learning_rate):\n",
    "    for layer in range(1, len(network) + 1):\n",
    "        network[layer]['W'] -= learning_rate * stored_grads[f'dW_{layer}']\n",
    "        network[layer]['b'] -= learning_rate * stored_grads[f'db_{layer}']\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "racial-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "focused-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y, network):\n",
    "    \n",
    "    stored_cost = []\n",
    "    \n",
    "    for epoch in range(HYPER_PARAMS['epochs']):\n",
    "        y_pred, cache = full_forward_pass(X, network)\n",
    "        cost = compute_cross_entropy_cost(y_pred, y)\n",
    "        stored_cost.append(cost)\n",
    "        stored_grads = full_backward_pass(y_pred, y, cache, network)\n",
    "        network = update_network(network, stored_grads, HYPER_PARAMS['learning_rate'])\n",
    "    return network, stored_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-freeze",
   "metadata": {},
   "source": [
    "### Resources\n",
    "This notebook has been inspired by the Towards Data Science post [Let’s code a Neural Network in plain NumPy](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795).\n",
    "\n",
    "Additional resources include:\n",
    "\n",
    "* [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/).\n",
    "* [Creating a Neural Network from Scratch in Python: Multi-class Classification](https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-multi-class-classification/).\n",
    "* [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/).\n",
    "* [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-income",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
