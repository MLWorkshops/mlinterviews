{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subjective-seafood",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial - Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ranging-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ignored-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 2\n",
    "LAYER_SIZES = [5, 5, 1]\n",
    "LAYER_SIZES.insert(0, INPUT_SIZE)\n",
    "LAYER_ACTIVATIONS = ['relu', 'relu', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hungarian-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network():\n",
    "    architecture = {}\n",
    "    for layer in range(1, len(LAYER_SIZES)):\n",
    "        architecture[f'layer_{layer}'] = {\n",
    "            'w': np.random.randn(LAYER_SIZES[layer],\n",
    "                                 LAYER_SIZES[layer-1]) * 1,\n",
    "            'b': np.zeros(layer),\n",
    "            'activation': LAYER_ACTIVATIONS[layer-1]\n",
    "        }\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alike-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_1': {'activation': 'relu',\n",
      "             'b': array([0.]),\n",
      "             'w': array([[ 0.16265908, -0.12909023],\n",
      "       [ 0.81427627,  1.31800588],\n",
      "       [-0.01846787, -0.94813517],\n",
      "       [-1.12477662, -0.01261812],\n",
      "       [ 0.21627657,  0.94370082]])},\n",
      " 'layer_2': {'activation': 'relu',\n",
      "             'b': array([0., 0.]),\n",
      "             'w': array([[-1.44603078, -1.00208109,  0.66925218, -0.07593184,  1.23347454],\n",
      "       [-0.44357222, -1.15782194, -0.90132038, -0.30412108, -0.07512328],\n",
      "       [-0.62317037, -1.6291713 , -1.46889242,  2.080184  , -0.2964723 ],\n",
      "       [-1.42418382, -1.33797094, -0.55368748, -1.7519944 , -0.11157937],\n",
      "       [ 2.68161409,  0.97724664, -0.60448605,  0.11955348, -1.56750555]])},\n",
      " 'layer_3': {'activation': 'sigmoid',\n",
      "             'b': array([0., 0., 0.]),\n",
      "             'w': array([[ 0.34481762,  0.31121743, -1.09858042, -1.39811787, -0.55488354]])}}\n"
     ]
    }
   ],
   "source": [
    "network = initialize_network()\n",
    "pprint.pprint(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interesting-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(Z):\n",
    "    activation = 1/(1 + np.exp(-1*Z))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(Z):\n",
    "    activation = np.exp(Z) / np.sum(np.exp(Z))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "informational-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(Z):\n",
    "    activation = np.max(0, Z)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "driving-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_sigmoid(dA, Z):\n",
    "    sigmoid = sigmoid_activation(Z)\n",
    "    dZ = dA * sigmoid * (1 - sigmoid)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "valid-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_softmax(dA, Z):\n",
    "    softmax = softmax_activation(Z)\n",
    "    softmax_matrix = np.tile(softmax)\n",
    "    dZ = np.diag(softmax) - (softmax_matrix*np.transpose(softmax_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "valid-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_relu(dA, Z):\n",
    "    dZ = np.copy(dA)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "marked-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_map = {\n",
    "    'sigmoid': sigmoid_activation,\n",
    "    'relu': relu_activation,\n",
    "    'softmax': softmax_activation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "animal-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ_map = {\n",
    "    'sigmoid': dZ_sigmoid,\n",
    "    'relu': dZ_relu,\n",
    "    'softmax': dZ_softmax\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "combined-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_forward_pass(A_previous, W, b, activation):\n",
    "    try:\n",
    "        act_function = act_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The activation {activation} is not recognized.\\nIt must be one of the following: {list(act_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    Z = np.dot(W, A_previous) + b\n",
    "    A = act_function(Z)\n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "brief-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_pass(X, network):\n",
    "    \n",
    "    cache = {}\n",
    "    A = X\n",
    "    \n",
    "    for layer in range(1, len(network) + 1):\n",
    "        \n",
    "        A_previous = A\n",
    "        A, Z = single_forward_pass(A_previous, network[layer]['W'], network[layer]['b'], network[layer]['activation'])\n",
    "        \n",
    "        cache[f'A_{layer-1}'] = A_previous\n",
    "        cache[f'Z_{layer}'] = Z\n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "searching-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_cost(y_pred, y):\n",
    "    \n",
    "    cost = np.sum(-1*(y * np.log(y_pred)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ignored-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backward_pass():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "downtown-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_pass():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "studied-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-invite",
   "metadata": {},
   "source": [
    "### Resources\n",
    "This notebook has been inspired by the Towards Data Science post [Letâ€™s code a Neural Network in plain NumPy](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795).\n",
    "\n",
    "* [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-shuttle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
