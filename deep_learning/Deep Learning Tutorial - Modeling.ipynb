{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blond-community",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial - Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "described-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "younger-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 2\n",
    "NUM_CLASSES = 7\n",
    "LAYER_SIZES = [5, 5]\n",
    "\n",
    "LAYER_SIZES.insert(0, INPUT_SIZE)\n",
    "\n",
    "LAYER_SIZES.append(NUM_CLASSES)\n",
    "\n",
    "LAYER_ACTIVATIONS = ['relu', 'relu', 'softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "widespread-prediction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 5, 7]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAYER_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "controlled-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network():\n",
    "    architecture = {}\n",
    "    for layer in range(1, len(LAYER_SIZES)):\n",
    "        architecture[f'layer_{layer}'] = {\n",
    "            'w': np.random.randn(LAYER_SIZES[layer],\n",
    "                                 LAYER_SIZES[layer-1]) * 1,\n",
    "            'b': np.zeros(LAYER_SIZES[layer]),\n",
    "            'activation': LAYER_ACTIVATIONS[layer-1]\n",
    "        }\n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "tough-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_1': {'activation': 'relu',\n",
      "             'b': array([0., 0., 0., 0., 0.]),\n",
      "             'w': array([[ 1.44419796,  1.16303701],\n",
      "       [-2.00961116,  0.28431591],\n",
      "       [ 1.01256116, -2.26256021],\n",
      "       [ 0.61697781,  1.09119743],\n",
      "       [-1.04527511, -0.05313304]])},\n",
      " 'layer_2': {'activation': 'relu',\n",
      "             'b': array([0., 0., 0., 0., 0.]),\n",
      "             'w': array([[-1.45764665, -1.06854323,  0.62869694,  0.47899202,  0.66973295],\n",
      "       [-1.13236351, -0.26974942,  0.89164398, -0.2906901 ,  0.68116742],\n",
      "       [-0.39036184,  0.44679897,  0.28661447, -1.21811627,  1.04665421],\n",
      "       [ 0.76816251, -0.64380966, -0.20099761,  0.20128848,  0.56124158],\n",
      "       [-0.25497632,  1.16944933, -0.98148751, -0.47203194,  2.00213812]])},\n",
      " 'layer_3': {'activation': 'softmax',\n",
      "             'b': array([0., 0., 0., 0., 0., 0., 0.]),\n",
      "             'w': array([[-1.84942065, -0.41903513,  1.58773187, -0.0658918 ,  0.29928075],\n",
      "       [-1.46360093, -0.44708949, -0.5382321 ,  0.66107264, -0.47293964],\n",
      "       [ 0.29483934, -0.38366907,  1.78133789,  0.84216707, -0.12126449],\n",
      "       [-1.20766949, -0.79273193,  1.60013227,  2.03454865,  0.18279269],\n",
      "       [-2.12084957,  0.79427228, -3.28787828,  0.60990599,  1.1154176 ],\n",
      "       [ 0.2203105 , -0.62157843,  0.95866364,  1.56362275,  0.51130355],\n",
      "       [-2.72826901, -0.37768974, -0.17123543, -0.35923885,  0.19010226]])}}\n"
     ]
    }
   ],
   "source": [
    "network = initialize_network()\n",
    "pprint.pprint(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "biblical-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(Z):\n",
    "    activation = 1/(1 + np.exp(-1*Z))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(Z):\n",
    "    activation = np.exp(Z) / np.sum(np.exp(Z))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recreational-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(Z):\n",
    "    activation = np.max(0, Z)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noted-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_sigmoid(dA, Z):\n",
    "    sigmoid = sigmoid_activation(Z)\n",
    "    dZ = dA * sigmoid * (1 - sigmoid)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "skilled-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_softmax(Z):\n",
    "    softmax = softmax_activation(Z)\n",
    "    softmax_matrix = np.tile(softmax)\n",
    "    dZ = np.diag(softmax) - (softmax_matrix*np.transpose(softmax_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "precise-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_relu(dA, Z):\n",
    "    dZ = np.copy(dA)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "altered-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_map = {\n",
    "    'sigmoid': sigmoid_activation,\n",
    "    'relu': relu_activation,\n",
    "    'softmax': softmax_activation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "guilty-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ_map = {\n",
    "    'sigmoid': dZ_sigmoid,\n",
    "    'relu': dZ_relu,\n",
    "    'softmax': dZ_softmax\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tender-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_forward_pass(A_previous, W, b, activation):\n",
    "    try:\n",
    "        act_function = act_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The activation {activation} is not recognized.\\nIt must be one of the following: {list(act_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    Z = np.dot(W, A_previous) + b\n",
    "    A = act_function(Z)\n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "opened-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_pass(X, network):\n",
    "    \n",
    "    cache = {}\n",
    "    A = X\n",
    "    \n",
    "    for layer in range(1, len(network) + 1):\n",
    "        \n",
    "        A_previous = A\n",
    "        A, Z = single_forward_pass(A_previous, network[layer]['W'], network[layer]['b'], network[layer]['activation'])\n",
    "        \n",
    "        cache[f'A_{layer-1}'] = A_previous\n",
    "        cache[f'Z_{layer}'] = Z\n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "viral-maximum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layer_3\n",
      "1 layer_2\n",
      "0 layer_1\n"
     ]
    }
   ],
   "source": [
    "for prev, current in reversed(list(enumerate(network))):\n",
    "    print(prev, current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "instant-federation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for layer in reversed(range(1, len(network) + 1)):\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "native-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy_cost(y_pred, y):\n",
    "    \n",
    "    cost = np.sum(-1*(y * np.log(y_pred)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "polished-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backward_pass(dA, W, b, Z, A_previous, activation):\n",
    "    \n",
    "    try:\n",
    "        backprop_activation = dZ_map[activation]\n",
    "    except KeyError:\n",
    "        print(f'The backprop activation {activation} is not recognized.\\nIt must be one of the following: {list(dZ_map.keys())}')\n",
    "        return None\n",
    "    \n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    if activations == 'softmax':\n",
    "        dZ = backprop_activation(Z)\n",
    "    else:\n",
    "        dZ = backprop_activation(dA, Z)\n",
    "    \n",
    "    dW = np.dot(dZ, np.transpose(A_previous)) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_previous = np.dot(np.transpose(dW), dZ)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "national-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_pass(y_pred, y, cache, network):\n",
    "    \n",
    "    stored_grads = {}\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    dA_previous = A - y\n",
    "    \n",
    "    for layer in reversed(range(1, len(network) + 1)):\n",
    "        activation = network[layer]['activation']\n",
    "        layer_previous = layer - 1\n",
    "        \n",
    "        dA = dA_previous\n",
    "        \n",
    "        A_previous = cache[f'A_{layer_previous}']\n",
    "        Z = cache[f'Z_{layer}']\n",
    "        W = network[layer]['W']\n",
    "        b = network[layer]['b']\n",
    "        \n",
    "        dA_previous, dW, db = single_backward_pass(dA, W, b, Z, A_previous, activation)\n",
    "        stored_grads[f'dW_{layer}'] = dW\n",
    "        stored_grads[f'db_{layer}'] = db\n",
    "        \n",
    "    return stored_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numerical-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-upset",
   "metadata": {},
   "source": [
    "### Resources\n",
    "This notebook has been inspired by the Towards Data Science post [Let’s code a Neural Network in plain NumPy](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795).\n",
    "\n",
    "* [The Softmax Function Derivative (Part 1)](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-physics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
